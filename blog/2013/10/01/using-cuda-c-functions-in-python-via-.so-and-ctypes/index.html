<!DOCTYPE html>
<html><head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="/css/bootstrap.min.css" rel="stylesheet" type="text/css" media="all">
  <link href="/css/style.css" rel="stylesheet" type="text/css" media="all">
  <link href="/css/custom.css" rel="stylesheet" type="text/css" media="all">
  <script src="/js/bootstrap.bundle.min.js"></script>
  <link rel="apple-touch-icon" href="/logo.png" type="image/png">
  <link rel="icon" href="/img/logo.svg" type="image/svg+xml">
  <link rel="icon" href="/img/logo.png" type="image/png">
  <title>Bikulov&#39;s blog</title>
</head><body>
<header>
  <nav class="navbar navbar-expand navbar-dark volukib-header">
    <div class="container-fluid">
      <a class="navbar-brand" href="https://bikulov.org/"><img src="/img/logo.png" class="volukib-logo" alt="logo"> Bikulov&#39;s blog</a>
      <ul class="navbar-nav">
        
        <li class="nav-item">
          
          <a class="nav-link text-uppercase " href="/">Home</a>
        </li>
        
        <li class="nav-item">
          
          <a class="nav-link text-uppercase " href="/resume/">Resume</a>
        </li>
        
      </ul>
    </div>
  </nav>
</header><div class="container">
<div class="row text-center mb-5">
  <div class="col">
    <h1>Using CUDA C&#43;&#43; functions in Python via `*.so` and ctypes</h1>
    <div>2013-10-01</div>
    <div>
      
      <a class="" href="/tags/cuda">#cuda</a>
      
      <a class="" href="/tags/howto">#howto</a>
      
      <a class="" href="/tags/c&#43;&#43;">#c&#43;&#43;</a>
      
      <a class="" href="/tags/bash">#bash</a>
      
      <a class="" href="/tags/python">#python</a>
      
    </div>
  </div>
</div>
<div class="row">
  <div class="col">
    <p>I&rsquo;d like to show how to use HPC part written on C++ with CUDA in Python code. So, every heavy part may be done on GPU with CUDA, all gluing tasks (with beautiful matplotlib plots) are done on CPU with Python.</p>
<p>We will use shared object, compiled from C++ CUDA code in Python. The only uncertain part here is conversation of types from «high-level» Python ones to «low-level» C++ ones. We will write application for parallel calculation of elementwise sum for two arrays.</p>
<p>First, CUDA code. CUDA kernel <code>cuda_sum_kernel</code> is doing job on GPU, wrapper <code>cuda_sum</code> prepares arrays for GPU and frees memory after calculation is done. Note <code>extern &quot;C&quot;</code> line. It is important for correct  function name in the compiled shared object later.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#75715e">#include</span> <span style="color:#75715e">&lt;cuda.h&gt;</span><span style="color:#75715e">
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">#include</span> <span style="color:#75715e">&lt;cuda_runtime_api.h&gt;</span><span style="color:#75715e">
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span>__global__ <span style="color:#66d9ef">void</span> <span style="color:#a6e22e">cuda_sum_kernel</span>(<span style="color:#66d9ef">float</span> <span style="color:#f92672">*</span>a, <span style="color:#66d9ef">float</span> <span style="color:#f92672">*</span>b, <span style="color:#66d9ef">float</span> <span style="color:#f92672">*</span>c, size_t size)
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>    size_t idx <span style="color:#f92672">=</span> blockIdx.x <span style="color:#f92672">*</span> blockDim.x <span style="color:#f92672">+</span> threadIdx.x;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (idx <span style="color:#f92672">&gt;=</span> size) {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span>;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    c[idx] <span style="color:#f92672">=</span> a[idx] <span style="color:#f92672">+</span> b[idx];
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">extern</span> <span style="color:#e6db74">&#34;C&#34;</span> {
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">void</span> <span style="color:#a6e22e">cuda_sum</span>(<span style="color:#66d9ef">float</span> <span style="color:#f92672">*</span>a, <span style="color:#66d9ef">float</span> <span style="color:#f92672">*</span>b, <span style="color:#66d9ef">float</span> <span style="color:#f92672">*</span>c, size_t size)
</span></span><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">float</span> <span style="color:#f92672">*</span>d_a, <span style="color:#f92672">*</span>d_b, <span style="color:#f92672">*</span>d_c;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    cudaMalloc((<span style="color:#66d9ef">void</span> <span style="color:#f92672">**</span>)<span style="color:#f92672">&amp;</span>d_a, size <span style="color:#f92672">*</span> <span style="color:#66d9ef">sizeof</span>(<span style="color:#66d9ef">float</span>));
</span></span><span style="display:flex;"><span>    cudaMalloc((<span style="color:#66d9ef">void</span> <span style="color:#f92672">**</span>)<span style="color:#f92672">&amp;</span>d_b, size <span style="color:#f92672">*</span> <span style="color:#66d9ef">sizeof</span>(<span style="color:#66d9ef">float</span>));
</span></span><span style="display:flex;"><span>    cudaMalloc((<span style="color:#66d9ef">void</span> <span style="color:#f92672">**</span>)<span style="color:#f92672">&amp;</span>d_c, size <span style="color:#f92672">*</span> <span style="color:#66d9ef">sizeof</span>(<span style="color:#66d9ef">float</span>));
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    cudaMemcpy(d_a, a, size <span style="color:#f92672">*</span> <span style="color:#66d9ef">sizeof</span>(<span style="color:#66d9ef">float</span>), cudaMemcpyHostToDevice);
</span></span><span style="display:flex;"><span>    cudaMemcpy(d_b, b, size <span style="color:#f92672">*</span> <span style="color:#66d9ef">sizeof</span>(<span style="color:#66d9ef">float</span>), cudaMemcpyHostToDevice);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    cuda_sum_kernel <span style="color:#f92672">&lt;&lt;&lt;</span> ceil(size <span style="color:#f92672">/</span> <span style="color:#ae81ff">256.0</span>), <span style="color:#ae81ff">256</span> <span style="color:#f92672">&gt;&gt;&gt;</span> (d_a, d_b, d_c, size);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    cudaMemcpy(c, d_c, size <span style="color:#f92672">*</span> <span style="color:#66d9ef">sizeof</span>(<span style="color:#66d9ef">float</span>), cudaMemcpyDeviceToHost);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    cudaFree(d_a);
</span></span><span style="display:flex;"><span>    cudaFree(d_b);
</span></span><span style="display:flex;"><span>    cudaFree(d_c);
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>Compile it to <code>*.so</code> file with nvcc compiler:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>/usr/local/cuda/bin/nvcc -Xcompiler -fPIC -shared -o cuda_sum.so cuda_sum.cu
</span></span></code></pre></div><p>The last part is to use function <code>cuda_sum</code> from created <code>cuda_sum.so</code> file in Python script. Example (with comments):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> ctypes
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> ctypes <span style="color:#f92672">import</span> <span style="color:#f92672">*</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># extract cuda_sum function pointer in the shared object cuda_sum.so</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_cuda_sum</span>():
</span></span><span style="display:flex;"><span>    dll <span style="color:#f92672">=</span> ctypes<span style="color:#f92672">.</span>CDLL(<span style="color:#e6db74">&#39;./cuda_sum.so&#39;</span>, mode<span style="color:#f92672">=</span>ctypes<span style="color:#f92672">.</span>RTLD_GLOBAL)
</span></span><span style="display:flex;"><span>    func <span style="color:#f92672">=</span> dll<span style="color:#f92672">.</span>cuda_sum
</span></span><span style="display:flex;"><span>    func<span style="color:#f92672">.</span>argtypes <span style="color:#f92672">=</span> [POINTER(c_float), POINTER(c_float), POINTER(c_float), c_size_t]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> func
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># create __cuda_sum function with get_cuda_sum()</span>
</span></span><span style="display:flex;"><span>__cuda_sum <span style="color:#f92672">=</span> get_cuda_sum()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># convenient python wrapper for __cuda_sum</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># it does all job with types convertation</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># from python ones to C++ ones</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">cuda_sum</span>(a, b, c, size):
</span></span><span style="display:flex;"><span>    a_p <span style="color:#f92672">=</span> a<span style="color:#f92672">.</span>ctypes<span style="color:#f92672">.</span>data_as(POINTER(c_float))
</span></span><span style="display:flex;"><span>    b_p <span style="color:#f92672">=</span> b<span style="color:#f92672">.</span>ctypes<span style="color:#f92672">.</span>data_as(POINTER(c_float))
</span></span><span style="display:flex;"><span>    c_p <span style="color:#f92672">=</span> c<span style="color:#f92672">.</span>ctypes<span style="color:#f92672">.</span>data_as(POINTER(c_float))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    __cuda_sum(a_p, b_p, c_p, size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># testing, sum of two arrays of ones and output head part of resulting array</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#39;__main__&#39;</span>:
</span></span><span style="display:flex;"><span>    size<span style="color:#f92672">=</span>int(<span style="color:#ae81ff">1024</span><span style="color:#f92672">*</span><span style="color:#ae81ff">1024</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    a <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ones(size)<span style="color:#f92672">.</span>astype(<span style="color:#e6db74">&#39;float32&#39;</span>)
</span></span><span style="display:flex;"><span>    b <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ones(size)<span style="color:#f92672">.</span>astype(<span style="color:#e6db74">&#39;float32&#39;</span>)
</span></span><span style="display:flex;"><span>    c <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(size)<span style="color:#f92672">.</span>astype(<span style="color:#e6db74">&#39;float32&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    cuda_sum(a, b, c, size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    print c[:<span style="color:#ae81ff">10</span>]
</span></span></code></pre></div><p>Now you can desing the code above into full-featured Python module.</p>

  </div>
</div>

  </div><div class="container-fluid volukib-footer p-2">
  <footer class="text-center">
    <a class="text-decoration-none text-light" href="https://bikulov.org/">Bikulov&#39;s blog 2022</a>
  </footer>
</div></body>

</html>